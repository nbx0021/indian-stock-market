{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad48e0c0-7aed-43fb-9cab-4d9ac3c29e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Setup & Load Data**\n",
    "\n",
    "\"Pandas Bridge\" method because of limitation of CE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d241eef-3898-4b03-977f-f412dead315b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Get the path dynamically\n",
    "current_folder = os.getcwd()\n",
    "\n",
    "# 2. Go UP one level to the main repo folder (using 'dirname')\n",
    "# This takes us from '.../notebooks' to '.../indian-stock-market'\n",
    "repo_root = os.path.dirname(current_folder)\n",
    "\n",
    "# 3. Construct the correct path\n",
    "csv_path = f\"{repo_root}/data/nifty50_sectors.csv\"\n",
    "\n",
    "# print(f\"Looking for file at: {csv_path}\")\n",
    "\n",
    "# 4. Check if file exists before reading (Debugging)\n",
    "if os.path.exists(csv_path):\n",
    "    print(\"‚úÖ File found! Loading...\")\n",
    "    pdf = pd.read_csv(csv_path)\n",
    "    df = spark.createDataFrame(pdf)\n",
    "    \n",
    "    # Ensure Date is actually a Date type\n",
    "    df = df.withColumn(\"Date\", F.to_date(F.col(\"Date\")))\n",
    "    # Rename 'Close' to 'Price' to match the rest of the analysis code\n",
    "    df = df.withColumnRenamed(\"Close\", \"Price\")\n",
    "\n",
    "    df.printSchema()\n",
    "    display(df.limit(5))\n",
    "else:\n",
    "    print(\"‚ùå File still NOT found.\")\n",
    "    print(\"üëâ ACTION REQUIRED: Go to the 'Repos' menu on the left, click your repo, and hit the 'Pull' button to sync the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00bc6ce9-a14b-4a4e-acd8-bf679e7b523d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Calculate Daily Returns (The \"Growth\")**\n",
    "\n",
    "We use a Window Function to look at the \"Previous Day's Price\" to see how much we gained or lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "694e293e-172a-402a-9857-bc5e1afc7026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7131f7a4-41a5-4ca2-bce0-2ae20d93b852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Calculate Volatility (The \"Risk\")** \n",
    "\n",
    "Standard Deviation of returns tells us how \"scary\" a stock is.\n",
    "\n",
    "High Volatility = Stock jumps up and down wildly (Riskier).\n",
    "\n",
    "Low Volatility = Stock is stable (Safe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a1ae58-1269-4ff5-b014-885ce7a5c747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "indian_sector_risk = spark.createDataFrame(\n",
    "    [\n",
    "        (\"IT\", 18.0),\n",
    "        (\"Banking\", 22.0),\n",
    "        (\"Pharma\", 16.0),\n",
    "        (\"Energy\", 20.0),\n",
    "        (\"FMCG\", 14.0),\n",
    "        (\"Metals\", 28.0),\n",
    "        (\"Auto\", 24.0)\n",
    "    ],\n",
    "    [\"Sector\", \"Indian_Std_Volatility_Pct\"]\n",
    ")\n",
    "# Calculate Annualized Volatility (Standard Deviation * sqrt(252 trading days))\n",
    "risk_profile = df_returns.groupBy(\"Ticker\", \"Sector\") \\\n",
    "    .agg(\n",
    "        F.round(F.stddev(\"Daily_Return\") * (252**0.5) * 100, 2).alias(\"Volatility_Annual_Pct\"),\n",
    "        F.round(F.avg(\"Daily_Return\") * 252 * 100, 2).alias(\"Return_Annual_Pct\")\n",
    "    ) \\\n",
    "    .orderBy(\"Volatility_Annual_Pct\", ascending=False)\n",
    "\n",
    "# Join with Indian sector standards\n",
    "risk_profile = risk_profile.join(\n",
    "    indian_sector_risk,\n",
    "    on=\"Sector\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Compare & classify risk level\n",
    "\n",
    "# Define thresholds:\n",
    "\n",
    "# Low Risk: < 80% of sector standard\n",
    "\n",
    "# Standard Risk: 80% ‚Äì 120%\n",
    "\n",
    "# High Risk: > 120%\n",
    "\n",
    "risk_profile = risk_profile.withColumn(\n",
    "    \"Risk_Comparison\",\n",
    "    F.when(\n",
    "        F.col(\"Volatility_Annual_Pct\") < F.col(\"Indian_Std_Volatility_Pct\") * 0.8,\n",
    "        \"Low\"\n",
    "    )\n",
    "    .when(\n",
    "        F.col(\"Volatility_Annual_Pct\") > F.col(\"Indian_Std_Volatility_Pct\") * 1.2,\n",
    "        \"High\"\n",
    "    )\n",
    "    .otherwise(\"Standard\")\n",
    ")\n",
    "\n",
    "# deviation from Indian standard\n",
    "risk_profile = risk_profile.withColumn(\n",
    "    \"Volatility_vs_India_Pct\",\n",
    "    F.round(\n",
    "        F.col(\"Volatility_Annual_Pct\") - F.col(\"Indian_Std_Volatility_Pct\"),\n",
    "        2\n",
    "    )\n",
    ")\n",
    "\n",
    "risk_profile = risk_profile.orderBy(\n",
    "    F.col(\"Volatility_Annual_Pct\").desc()\n",
    ")\n",
    "\n",
    "\n",
    "# Null values in a Volatility or Standard Deviation calculation usually happen for one reason: Insufficient Data.\n",
    "# solution\n",
    "# The \"Clean Up\" (Recommended)\n",
    "# Since you cannot analyze the risk of a stock that has no risk data, the standard practice is to simply remove those rows.\n",
    "\n",
    "# Remove rows where Volatility is NULL\n",
    "risk_profile_clean = risk_profile.dropna(subset=[\"Volatility_Annual_Pct\"])\n",
    "\n",
    "# Now sort and display\n",
    "risk_profile_clean = risk_profile_clean.orderBy(F.col(\"Volatility_Annual_Pct\").desc())\n",
    "\n",
    "display(risk_profile_clean)\n",
    "\n",
    "# display(risk_profile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465326a7-83bc-4f2f-985e-525daf3f6025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Technical Indicators (Moving Averages) **\n",
    "\n",
    "Traders use the 50-Day Moving Average (MA50) to see trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7878876-84a1-4ee0-9b5a-4eda1cd9b81c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769586133821}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Define Window (Current Day + 49 Previous Days = 50 Total)\n",
    "window_50 = Window.partitionBy(\"Ticker\").orderBy(\"Date\").rowsBetween(-49, 0)\n",
    "\n",
    "# 2. Calculate Technicals\n",
    "df_technical = df.withColumn(\"MA50\", F.avg(\"Price\").over(window_50))\n",
    "\n",
    "# 3. Get Latest Date (Dynamic for Automation)\n",
    "latest_date = df.select(F.max(\"Date\")).collect()[0][0]\n",
    "\n",
    "# 4. Generate Signals with Strength\n",
    "latest_signals = df_technical.filter(F.col(\"Date\") == latest_date) \\\n",
    "    .select(\"Ticker\", \"Price\", \"MA50\") \\\n",
    "    .withColumn(\"MA_Diff_Pct\", F.round((F.col(\"Price\") - F.col(\"MA50\")) / F.col(\"MA50\") * 100, 2)) \\\n",
    "    .withColumn(\n",
    "        \"Trend\",\n",
    "        F.when(F.col(\"MA_Diff_Pct\") > 2, \"Strong Bullish üöÄ\")      # Price is >2% above MA\n",
    "         .when(F.col(\"MA_Diff_Pct\") > 0, \"Weak Bullish ‚ÜóÔ∏è\")         # Price is slightly above MA\n",
    "         .when(F.col(\"MA_Diff_Pct\") < -2, \"Strong Bearish ü©∏\")     # Price is >2% below MA\n",
    "         .otherwise(\"Weak Bearish ‚ÜòÔ∏è\")                              # Price is slightly below MA\n",
    "    )\n",
    "\n",
    "display(latest_signals.orderBy(\"MA_Diff_Pct\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c73ac1-af53-4754-869f-3190edfb9160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Correlation Analysis and Cumulative Returns.**\n",
    "\n",
    "These answer the \"Big Picture\" questions:\n",
    "\n",
    "Correlation: \"If IT stocks crash, will Banking stocks crash too?\" (Hedging).\n",
    "\n",
    "Cumulative Return: \"If I invested ‚Çπ1 Lakh 5 years ago, who made me the most money?\" (Performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918e4504-8020-48a9-bb85-b8895e614efd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Correlation Matrix (Sector Rotation)**\n",
    "This calculates how much sectors move together.\n",
    "\n",
    "+1.0: They move identically (High Risk).\n",
    "\n",
    "-1.0: They move oppositely (Great for Hedging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b04afc6-1558-4fb0-821d-d9b12700edc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Pivot the data\n",
    "# We use 'Daily_Return' for correlation\n",
    "df_pivot = df_returns.groupBy(\"Date\").pivot(\"Ticker\").mean(\"Daily_Return\")\n",
    "\n",
    "# 2. Drop dates with nulls\n",
    "df_pivot = df_pivot.na.drop()\n",
    "\n",
    "# error fix in the code\n",
    "# Before: inputCols=[\"HDFCBANK.NS\"] -> Spark looks for a struct named HDFCBANK.\n",
    "\n",
    "# After: We pass the list of strings, but VectorAssembler handles the column selection internally. If the error persists specifically in the VectorAssembler line, try renaming the columns to remove the dot entirely\n",
    "\n",
    "# Rename columns to remove '.NS' (e.g., 'HDFCBANK.NS' -> 'HDFCBANK')\n",
    "for col_name in df_pivot.columns:\n",
    "    if \".\" in col_name:\n",
    "        df_pivot = df_pivot.withColumnRenamed(col_name, col_name.replace(\".\", \"_\"))\n",
    "\n",
    "# Now run correlation on new names (HDFCBANK_NS, etc.)\n",
    "stock_columns = [c for c in df_pivot.columns if c != \"Date\"]\n",
    "assembler = VectorAssembler(inputCols=stock_columns, outputCol=\"corr_features\")\n",
    "df_vector = assembler.transform(df_pivot)\n",
    "pearson_matrix = Correlation.corr(df_vector, \"corr_features\").head()[0]\n",
    "\n",
    "print(\"‚úÖ Correlation Matrix Calculated!\")\n",
    "# Convert Spark correlation matrix to pandas DataFrame for display\n",
    "corr_array = pearson_matrix.toArray()\n",
    "corr_df = pd.DataFrame(corr_array, columns=stock_columns, index=stock_columns)\n",
    "display(corr_df)\n",
    "\n",
    "# Save the Correlation Matrix for the Dashboard\n",
    "output_path_corr = f\"{repo_root}/data/stock_correlation.csv\"\n",
    "\n",
    "# Since it's already a Pandas DF, saving is easy\n",
    "corr_df.to_csv(output_path_corr)\n",
    "\n",
    "print(f\"‚úÖ Correlation Matrix saved to: {output_path_corr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eba4496d-8a9c-4638-bafb-5f499bac2bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Cumulative Return (The \"Wealth Chart\")**\n",
    "\n",
    "This shows the growth of an investment over time. We start everyone at 1.0 (or 100%) on Day 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e151fc7-b04d-4f09-9740-69f61fc1b78a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 1. Define the Indian Format Function\n",
    "def indian_format(num):\n",
    "    if num is None: return \"0\"\n",
    "    s = str(int(num))\n",
    "    if len(s) <= 3:\n",
    "        return s\n",
    "    else:\n",
    "        last_three = s[-3:]\n",
    "        rest = s[:-3]\n",
    "        parts = []\n",
    "        while len(rest) > 2:\n",
    "            parts.append(rest[-2:])\n",
    "            rest = rest[:-2]\n",
    "        if rest:\n",
    "            parts.append(rest)\n",
    "        return ','.join(reversed(parts)) + ',' + last_three\n",
    "\n",
    "indian_format_udf = udf(indian_format, StringType())\n",
    "\n",
    "# 2. Calculate Growth (Keep it Numeric!)\n",
    "df_growth = df_returns.withColumn(\"Log_Return\", F.log(F.col(\"Daily_Return\") + 1)) \\\n",
    "    .withColumn(\"Sum_Log_Return\", F.sum(\"Log_Return\").over(window_cum)) \\\n",
    "    .withColumn(\"Cumulative_Growth\", F.exp(\"Sum_Log_Return\")) \\\n",
    "    .withColumn(\"Investment_Value_Numeric\", F.round(F.col(\"Cumulative_Growth\") * 100000, 0)) # Keep raw number\n",
    "\n",
    "# 3. Get Latest Date Dynamically\n",
    "latest_date = df_growth.select(F.max(\"Date\")).collect()[0][0]\n",
    "\n",
    "# 4. Filter, Sort (Numeric), THEN Format\n",
    "leaderboard = df_growth.filter(F.col(\"Date\") == latest_date) \\\n",
    "    .orderBy(\"Investment_Value_Numeric\", ascending=False) \\\n",
    "    .withColumn(\"Current_Value_INR\", indian_format_udf(F.col(\"Investment_Value_Numeric\"))) \\\n",
    "    .select(\"Ticker\", \"Current_Value_INR\") # Only show the pretty string\n",
    "\n",
    "display(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4296930-9f08-4a02-819e-f5af2e78eb77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1. Setup Path\n",
    "current_folder = os.getcwd()\n",
    "repo_root = os.path.dirname(current_folder)\n",
    "data_folder = f\"{repo_root}/data\"\n",
    "\n",
    "print(f\"üìÇ Saving all analysis files to: {data_folder}\")\n",
    "\n",
    "# --- SAVE 1: Risk Profile (Volatility) ---\n",
    "try:\n",
    "    path_risk = f\"{data_folder}/stock_risk.csv\"\n",
    "    # Convert to Pandas and save\n",
    "    risk_profile.toPandas().to_csv(path_risk, index=False)\n",
    "    print(f\"‚úÖ Saved Risk Profile to: {path_risk}\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è 'risk_profile' table not found. (Did you run the Volatility cell?)\")\n",
    "\n",
    "# --- SAVE 2: Buy/Sell Signals (Indicators) ---\n",
    "try:\n",
    "    path_signals = f\"{data_folder}/stock_signals.csv\"\n",
    "    latest_signals.toPandas().to_csv(path_signals, index=False)\n",
    "    print(f\"‚úÖ Saved Signals to: {path_signals}\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è 'latest_signals' table not found. (Did you run the Indicator cell?)\")\n",
    "\n",
    "# --- SAVE 3: Growth Leaderboard ---\n",
    "try:\n",
    "    path_growth = f\"{data_folder}/stock_growth.csv\"\n",
    "    df_growth.toPandas().to_csv(path_growth, index=False)\n",
    "    print(f\"‚úÖ Saved Growth Data to: {path_growth}\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è 'df_growth' table not found. (Did you run the Cumulative Return cell?)\")\n",
    "\n",
    "print(\"\\nüöÄ DONE! Go to Git Menu -> Commit & Push now.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stock_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
